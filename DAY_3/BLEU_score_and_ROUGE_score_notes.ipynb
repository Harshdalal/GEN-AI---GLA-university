{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#BLEU score and ROUGE score\n",
        "\n",
        "two widely used metrics to evaluate text generation tasks like machine translation, summarization, and text generation.\n",
        "\n",
        "#ðŸ”µ BLEU Score (Bilingual Evaluation Understudy)\n",
        "\n",
        "ðŸ“Œ Purpose:\n",
        "\n",
        "Measures how close a machine-generated sentence is to a human reference sentence, primarily used in machine translation.\n",
        "\n",
        "#ðŸ§  How it works:\n",
        "\n",
        ">Compares n-grams (e.g., unigrams, bigrams, etc.) in the generated and reference text.\n",
        "\n",
        ">Calculates the precision of these n-grams.\n",
        "\n",
        ">Adds a brevity penalty to penalize too-short outputs.\n",
        "\n",
        "#ðŸ§® Formula (simplified):\n",
        "\n",
        "BLEU = BP Ã— exp(Î£ wâ‚™ Ã— log pâ‚™)\n",
        "\n",
        ">BP: Brevity penalty\n",
        "\n",
        ">p_n: Precision for n-gram (e.g., 1-gram, 2-gram, ...)\n",
        "\n",
        ">w_n: Weight (usually equal for all n-grams)\n",
        "\n",
        "âœ… Value Range:\n",
        "\n",
        "0 to 1, or 0 to 100 if scaled\n",
        "\n",
        "Higher is better\n",
        "\n",
        "\n",
        "âœ… Example:\n",
        "\n",
        "Reference: \"The cat is on the mat\"\n",
        "\n",
        "Prediction: \"The cat is on mat\"\n",
        "\n",
        "BLEU will detect missing word \"the\", lowering the score.\n",
        "\n",
        "#ðŸ”´ ROUGE Score (Recall-Oriented Understudy for Gisting Evaluation)\n",
        "ðŸ“Œ Purpose:\n",
        "\n",
        "Used mainly for summarization evaluation by comparing overlap between reference and generated summaries.\n",
        "\n",
        "Types:\n",
        "\n",
        ">ROUGE-N: Overlap of n-grams (like BLEU but recall-based)\n",
        "\n",
        ">ROUGE-L: Longest common subsequence (captures structure)\n",
        "\n",
        ">ROUGE-W: Weighted LCS\n",
        "\n",
        ">ROUGE-S: Skip bigrams\n",
        "\n",
        "#ðŸ§  How it works:\n",
        "\n",
        ">Measures recall, not precision.\n",
        "\n",
        ">Looks for how much of the reference is captured by the generated summary.\n",
        "\n",
        "âœ… Value Range:\n",
        "\n",
        "0 to 1, or 0% to 100%\n",
        "\n",
        "Higher = better match\n",
        "\n",
        "#ðŸ§ª Example (ROUGE):\n",
        "\n",
        "Reference: \"The cat is on the mat\"\n",
        "\n",
        "Prediction: \"The cat sits on mat\"\n",
        "\n",
        "ROUGE would count how many words (or bigrams) are matched.\n",
        "\n",
        "It measures recall: how much of the reference was covered.\n",
        "\n",
        "#ðŸ”„ BLEU vs ROUGE\n",
        "\n",
        "| Metric    | Focus     | Use Case            | Measures       | Strength            |\n",
        "| --------- | --------- | ------------------- | -------------- | ------------------- |\n",
        "| **BLEU**  | Precision | Machine Translation | n-gram overlap | Penalizes brevity   |\n",
        "| **ROUGE** | Recall    | Text Summarization  | n-gram & LCS   | Sensitive to recall |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gNsf4S7NnTGd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fHt3xGn-nPoy"
      },
      "outputs": [],
      "source": []
    }
  ]
}
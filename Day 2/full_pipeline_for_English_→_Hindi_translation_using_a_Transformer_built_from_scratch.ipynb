{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# full pipeline for English → Hindi translation using a Transformer built from scratch\n",
        "\n",
        "🔧 Overview:\n",
        "\n",
        "We will build a Transformer model using:\n",
        "\n",
        ">Custom Embeddings\n",
        "\n",
        ">Positional Encoding\n",
        "\n",
        ">Encoder & Decoder with Multi-Head Attention\n",
        "\n",
        ">Masking\n",
        "\n",
        ">Teacher Forcing\n",
        "\n",
        ">Training on a custom parallel English-Hindi dataset\n",
        "\n",
        "#✅ Step-by-Step Practical Structure:\n",
        "🔹 Step 1: Install Required Libraries"
      ],
      "metadata": {
        "id": "RAYwd_zfLTfc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wS4ZS4qYuVbo",
        "outputId": "316ceef9-7c4b-44f2-fc54-7fa3ed9f8efa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.73.1)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.7.14)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Requirement already satisfied: tensorflow-text in /usr/local/lib/python3.11/dist-packages (2.18.1)\n",
            "Requirement already satisfied: tensorflow<2.19,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-text) (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (4.14.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (1.73.1)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow<2.19,>=2.18.0->tensorflow-text) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow-text) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow-text) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow-text) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18.0->tensorflow-text) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18.0->tensorflow-text) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18.0->tensorflow-text) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18.0->tensorflow-text) (2025.7.14)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow-text) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow-text) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow-text) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow-text) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow-text) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow-text) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow-text) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow\n",
        "!pip install tensorflow-text"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#🔹 Step 2: Load English-Hindi Translation Dataset\n",
        "We will use a small sample for quick testing:"
      ],
      "metadata": {
        "id": "hsNafHSmME_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data for demo purposes\n",
        "english_sentences = [\n",
        "    \"hello\", \"how are you\", \"i am fine\", \"what is your name\", \"good morning\"\n",
        "]\n",
        "hindi_sentences = [\n",
        "    \"नमस्ते\", \"आप कैसे हैं\", \"मैं ठीक हूँ\", \"तुम्हारा नाम क्या है\", \"सुप्रभात\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "77JurOsHuk_p"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#🔹 Step 3: Tokenization\n"
      ],
      "metadata": {
        "id": "Qa3SndVUMH8s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "# -------------------------------\n",
        "# Step 1: Tokenize English Sentences\n",
        "# -------------------------------\n",
        "\n",
        "# Create a tokenizer for English with:\n",
        "# - No filtering of punctuation (`filters=''`)\n",
        "# - An out-of-vocabulary token to handle unseen words\n",
        "eng_tokenizer = Tokenizer(filters='', oov_token='<OOV>')\n",
        "\n",
        "# Fit the tokenizer on the list of English sentences\n",
        "eng_tokenizer.fit_on_texts(english_sentences)\n",
        "\n",
        "# Convert each English sentence into a sequence of integers (word indices)\n",
        "eng_tensor = eng_tokenizer.texts_to_sequences(english_sentences)\n",
        "\n",
        "# Pad sequences so that all input sentences have equal length\n",
        "# Padding is added to the end ('post') to preserve input ordering\n",
        "eng_tensor = pad_sequences(eng_tensor, padding='post')\n",
        "\n",
        "# -------------------------------\n",
        "# Step 2: Tokenize Hindi Sentences\n",
        "# -------------------------------\n",
        "\n",
        "# Create a tokenizer for Hindi with the same configuration\n",
        "hin_tokenizer = Tokenizer(filters='', oov_token='<OOV>')\n",
        "hin_tokenizer.fit_on_texts(hindi_sentences)\n",
        "hin_tensor = hin_tokenizer.texts_to_sequences(hindi_sentences)\n",
        "hin_tensor = pad_sequences(hin_tensor, padding='post')\n",
        "\n",
        "# -------------------------------\n",
        "# Step 3: Vocabulary Sizes\n",
        "# -------------------------------\n",
        "\n",
        "# Vocabulary size is the total number of unique tokens + 1 for padding (0 index)\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "hin_vocab_size = len(hin_tokenizer.word_index) + 1\n",
        "\n",
        "# Now `eng_tensor` and `hin_tensor` contain the integer-encoded and padded input/output data\n",
        "# These can now be used to train the sequence-to-sequence transformer model\n"
      ],
      "metadata": {
        "id": "wi5KRySlupX1"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#🔹 Step 4: Positional Encoding"
      ],
      "metadata": {
        "id": "t3LFt-8SMLWJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def positional_encoding(position, d_model):\n",
        "    \"\"\"\n",
        "    Compute the positional encoding matrix.\n",
        "\n",
        "    Args:\n",
        "        position: Maximum length of the sequence (e.g., 100).\n",
        "        d_model: Dimensionality of the embedding/hidden layer (e.g., 512).\n",
        "\n",
        "    Returns:\n",
        "        A tensor of shape (1, position, d_model) containing the positional encodings.\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1: Create a matrix of shape (position, 1)\n",
        "    # Each row corresponds to a position index (0 to position-1)\n",
        "    angle_rads = np.arange(position)[:, np.newaxis] / np.power(\n",
        "        10000,\n",
        "        (2 * (np.arange(d_model)[np.newaxis, :] // 2)) / np.float32(d_model)\n",
        "    )\n",
        "    # np.arange(d_model)[np.newaxis, :] creates a shape (1, d_model) row for dimension indices\n",
        "    # (// 2) groups even and odd dimensions for sin/cos separately\n",
        "\n",
        "    # Step 2: Apply sin to even indices in the array; 0, 2, 4, ...\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "    # Step 3: Apply cos to odd indices in the array; 1, 3, 5, ...\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    # Step 4: Add a batch dimension (1, position, d_model) and convert to Tensor\n",
        "    return tf.cast(angle_rads[np.newaxis, ...], dtype=tf.float32)\n"
      ],
      "metadata": {
        "id": "-q1F8NozuqD4"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#🔹 Step 5: Scaled Dot-Product Attention"
      ],
      "metadata": {
        "id": "CtO9JIszMNi3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask=None):\n",
        "    \"\"\"\n",
        "    Calculate the scaled dot-product attention.\n",
        "\n",
        "    Args:\n",
        "        q: query tensor of shape (..., seq_len_q, depth)\n",
        "        k: key tensor of shape (..., seq_len_k, depth)\n",
        "        v: value tensor of shape (..., seq_len_v, depth_v)\n",
        "        mask: Float tensor with shape broadcastable to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        output: Attention output after applying weights to V\n",
        "        attention_weights: Softmax weights used for attention\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1: Calculate the dot product between Q and K^T\n",
        "    print(\"Shape of q in scaled_dot_product_attention:\", tf.shape(q))\n",
        "    print(\"Shape of k in scaled_dot_product_attention:\", tf.shape(k))\n",
        "    print(\"Shape of v in scaled_dot_product_attention:\", tf.shape(v))\n",
        "    print(\"Shape of mask in scaled_dot_product_attention:\", tf.shape(mask) if mask is not None else \"None\")\n",
        "\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "    # Step 2: Scale the dot products by the square root of the dimension of the key vectors\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)       # depth of key vectors\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)  # Stabilize gradients\n",
        "\n",
        "    # Step 3: Add the mask (if any) to ignore padding positions or future tokens (in decoder)\n",
        "    if mask is not None:\n",
        "        # Use tf.where to apply the mask\n",
        "        scaled_attention_logits = tf.where(mask == 0, scaled_attention_logits, -1e9)\n",
        "\n",
        "\n",
        "    # Step 4: Apply softmax to get attention weights (probabilities)\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "    # Step 5: Multiply attention weights with values (V)\n",
        "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "    return output, attention_weights"
      ],
      "metadata": {
        "id": "FczYO7AburDA"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#🔹 Step 6: Multi-Head Attention"
      ],
      "metadata": {
        "id": "PhU5x8OJMQEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        \"\"\"\n",
        "        Initializes the MultiHeadAttention layer.\n",
        "\n",
        "        Args:\n",
        "            d_model: Dimension of the model (output of attention layer)\n",
        "            num_heads: Number of parallel attention heads\n",
        "        \"\"\"\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "\n",
        "        # Ensure d_model is divisible by number of heads (for equal splitting)\n",
        "        assert d_model % num_heads == 0\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        self.depth = d_model // num_heads  # Depth of each head\n",
        "\n",
        "        # Dense layers to transform input into Q, K, V matrices\n",
        "        self.wq = tf.keras.layers.Dense(d_model)  # Linear layer for query\n",
        "        self.wk = tf.keras.layers.Dense(d_model)  # Linear layer for key\n",
        "        self.wv = tf.keras.layers.Dense(d_model)  # Linear layer for value\n",
        "\n",
        "        # Final dense layer to combine all heads\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        \"\"\"\n",
        "        Splits the last dimension into (num_heads, depth) and rearranges to shape:\n",
        "        (batch_size, num_heads, seq_len, depth)\n",
        "        \"\"\"\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))  # reshape\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])  # move num_heads before seq_len\n",
        "\n",
        "    def call(self, v, k, q, mask):\n",
        "        \"\"\"\n",
        "        Forward pass of the MultiHeadAttention layer.\n",
        "\n",
        "        Args:\n",
        "            v: Value tensor\n",
        "            k: Key tensor\n",
        "            q: Query tensor\n",
        "            mask: Mask to apply (optional)\n",
        "\n",
        "        Returns:\n",
        "            output: Final attention output of shape (batch_size, seq_len_q, d_model)\n",
        "        \"\"\"\n",
        "        batch_size = tf.shape(q)[0]  # Get dynamic batch size\n",
        "\n",
        "        # Linear projections of Q, K, V\n",
        "        q = self.wq(q)  # (batch_size, seq_len_q, d_model)\n",
        "        k = self.wk(k)  # (batch_size, seq_len_k, d_model)\n",
        "        v = self.wv(v)  # (batch_size, seq_len_v, d_model)\n",
        "\n",
        "        print(\"Shape of q after dense:\", tf.shape(q))\n",
        "        print(\"Shape of k after dense:\", tf.shape(k))\n",
        "        print(\"Shape of v after dense:\", tf.shape(v))\n",
        "\n",
        "        # Split Q, K, V into multiple heads\n",
        "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "\n",
        "        print(\"Shape of q after split_heads:\", tf.shape(q))\n",
        "        print(\"Shape of k after split_heads:\", tf.shape(k))\n",
        "        print(\"Shape of v after split_heads:\", tf.shape(v))\n",
        "\n",
        "\n",
        "        # Apply scaled dot-product attention to each head\n",
        "        scaled_attention, _ = scaled_dot_product_attention(q, k, v, mask)\n",
        "        # Transpose back: (batch_size, seq_len_q, num_heads, depth)\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "\n",
        "        # Concatenate all heads together\n",
        "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.num_heads * self.depth))\n",
        "        # Final linear projection\n",
        "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "CUgNZGcyur4l"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#🔹 Step 7: Feed Forward Network"
      ],
      "metadata": {
        "id": "87xj0fZaMTGx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "    return tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(dff, activation='relu'),\n",
        "        tf.keras.layers.Dense(d_model)\n",
        "    ])\n"
      ],
      "metadata": {
        "id": "Z-cmDVwLus_n"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#🔹 Step 8: Encoder and Decoder Layers"
      ],
      "metadata": {
        "id": "9KxTWgzPMVMr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder Layer definition\n",
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff):\n",
        "        super().__init__()\n",
        "\n",
        "        # Multi-head self-attention mechanism\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        # Feed Forward Network (two dense layers)\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        # Layer normalization layers (helps with training stability)\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization()\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "    def call(self, x, mask):\n",
        "        # Self-attention over input (query, key, value all = x)\n",
        "        attn_output = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        # Add & Norm: Residual connection + normalization\n",
        "        out1 = self.layernorm1(x + attn_output)\n",
        "\n",
        "        # Feed Forward Network\n",
        "        ffn_output = self.ffn(out1)\n",
        "\n",
        "        # Add & Norm again\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "\n",
        "# Decoder Layer definition\n",
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff):\n",
        "        super().__init__()\n",
        "\n",
        "        # First multi-head attention (masked self-attention)\n",
        "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        # Second multi-head attention (cross-attention: query = decoder, key/value = encoder)\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        # Feed Forward Network\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        # Layer normalizations for each sub-layer\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization()\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization()\n",
        "        self.layernorm3 = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "    def call(self, x, enc_output, look_ahead_mask, padding_mask):\n",
        "        # First attention block: masked self-attention\n",
        "        attn1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
        "        out1 = self.layernorm1(attn1 + x)  # Add & Norm\n",
        "\n",
        "        # Second attention block: cross-attention with encoder output\n",
        "        attn2 = self.mha2(enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
        "        out2 = self.layernorm2(attn2 + out1)  # Add & Norm\n",
        "\n",
        "        # Feed Forward Network\n",
        "        ffn_output = self.ffn(out2)\n",
        "\n",
        "        # Final Add & Norm\n",
        "        return self.layernorm3(ffn_output + out2)\n"
      ],
      "metadata": {
        "id": "EGLrXJ83uurT"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#🔹 Step 9: Build Transformer Model"
      ],
      "metadata": {
        "id": "sl65gqs0MXk0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding):\n",
        "        super().__init__()\n",
        "\n",
        "        # Embedding layer to convert token indices to dense vectors\n",
        "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "\n",
        "        # Positional encoding helps model understand position of words in sequence\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "\n",
        "        # Stack of encoder layers\n",
        "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff) for _ in range(num_layers)]\n",
        "\n",
        "        # Dropout layer for regularization\n",
        "        self.dropout = tf.keras.layers.Dropout(0.1)\n",
        "\n",
        "    def call(self, x, mask):\n",
        "        seq_len = tf.shape(x)[1]  # Length of input sequence\n",
        "\n",
        "        # Token embeddings + positional encoding\n",
        "        x = self.embedding(x)  # (batch_size, seq_len, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.embedding.output_dim, tf.float32)) # Scale embeddings\n",
        "        x += self.pos_encoding[:, :seq_len, :]  # Add positional information\n",
        "\n",
        "        x = self.dropout(x)  # Apply dropout\n",
        "\n",
        "        # Pass through all encoder layers\n",
        "        for enc_layer in self.enc_layers:\n",
        "            x = enc_layer(x, mask)  # Apply self-attention and feed-forward\n",
        "\n",
        "        return x  # Output of the encoder (batch_size, input_seq_len, d_model)\n",
        "\n",
        "\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding):\n",
        "        super().__init__()\n",
        "\n",
        "        # Embedding for target tokens\n",
        "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "\n",
        "        # Positional encoding for decoder input\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "\n",
        "        # Stack of decoder layers\n",
        "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff) for _ in range(num_layers)]\n",
        "\n",
        "        # Dropout for regularization\n",
        "        self.dropout = tf.keras.layers.Dropout(0.1)\n",
        "\n",
        "    def call(self, x, enc_output, look_ahead_mask, padding_mask):\n",
        "        seq_len = tf.shape(x)[1]  # Length of target sequence\n",
        "\n",
        "        # Token embeddings + positional encoding\n",
        "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.embedding.output_dim, tf.float32)) # Scale embeddings\n",
        "        x += self.pos_encoding[:, :seq_len, :]  # Add positional information\n",
        "\n",
        "        x = self.dropout(x)  # Apply dropout\n",
        "\n",
        "        # Pass through each decoder layer\n",
        "        for dec_layer in self.dec_layers:\n",
        "            x = dec_layer(x, enc_output, look_ahead_mask, padding_mask)\n",
        "\n",
        "        return x  # Output of the decoder (batch_size, target_seq_len, d_model)"
      ],
      "metadata": {
        "id": "7q6Kv3Uyuweo"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#🔹 Step 10: Final Transformer"
      ],
      "metadata": {
        "id": "JgFpg67NMZ4j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff,\n",
        "                 input_vocab_size, target_vocab_size, pe_input, pe_target):\n",
        "        super().__init__()\n",
        "\n",
        "        # Encoder to process the input sequence\n",
        "        self.encoder = Encoder(\n",
        "            num_layers, d_model, num_heads, dff,\n",
        "            input_vocab_size, pe_input\n",
        "        )\n",
        "\n",
        "        # Decoder to process the target sequence using encoder output\n",
        "        self.decoder = Decoder(\n",
        "            num_layers, d_model, num_heads, dff,\n",
        "            target_vocab_size, pe_target\n",
        "        )\n",
        "\n",
        "        # Final dense layer to project decoder output to vocabulary logits\n",
        "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "    def call(self, inp, tar, look_ahead_mask, dec_padding_mask):\n",
        "        # Create padding mask for the encoder input\n",
        "        enc_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "        # Pass input through the encoder\n",
        "        enc_output = self.encoder(inp, enc_padding_mask)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        # Pass target tokens, encoder output, and masks through decoder\n",
        "        dec_output = self.decoder(\n",
        "            tar, enc_output,\n",
        "            look_ahead_mask, dec_padding_mask\n",
        "        )  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "        # Output layer converts decoder output into logits over target vocabulary\n",
        "        return self.final_layer(dec_output)  # (batch_size, target_seq_len, target_vocab_size)\n",
        "\n",
        "def create_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "    # add extra dimensions to add the padding to the attention logits.\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
      ],
      "metadata": {
        "id": "y4t_oYVAuxdl"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf8a7061"
      },
      "source": [
        "# Task\n",
        "Add code to the notebook to translate an English sentence to Hindi using the trained Transformer model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d50ba60"
      },
      "source": [
        "## Create a function to preprocess the input english sentence\n",
        "\n",
        "### Subtask:\n",
        "Create a function that preprocesses a raw English sentence for the Transformer model. This involves tokenizing the sentence and padding it to the maximum input sequence length.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1d5d375"
      },
      "source": [
        "**Reasoning**:\n",
        "Define a function to preprocess the English sentence by tokenizing and padding it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "299b282c"
      },
      "source": [
        "def preprocess_sentence(sentence):\n",
        "    \"\"\"\n",
        "    Tokenizes and pads a single English sentence for the Transformer model.\n",
        "\n",
        "    Args:\n",
        "        sentence: The raw English sentence string.\n",
        "\n",
        "    Returns:\n",
        "        A TensorFlow tensor containing the padded tokenized sentence.\n",
        "    \"\"\"\n",
        "    # Tokenize the sentence using the pre-fitted English tokenizer\n",
        "    sentence_sequence = eng_tokenizer.texts_to_sequences([sentence])\n",
        "\n",
        "    # Pad the sequence to the maximum input length\n",
        "    padded_sequence = pad_sequences(sentence_sequence, maxlen=pe_input, padding='post')\n",
        "\n",
        "    # Convert the padded sequence to a TensorFlow tensor\n",
        "    return tf.cast(padded_sequence, dtype=tf.int64)\n"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e04b26c"
      },
      "source": [
        "## Create a function to generate the translation\n",
        "\n",
        "### Subtask:\n",
        "Create a function that takes a preprocessed English input tensor and the Transformer model and generates the Hindi translation using greedy decoding.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6320adf5"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the `translate` function which implements greedy decoding to generate the Hindi translation from the English input tensor using the Transformer model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9f919dae"
      },
      "source": [
        "def translate(input_sentence_tensor, transformer):\n",
        "    \"\"\"\n",
        "    Translates an English input tensor to Hindi using greedy decoding.\n",
        "\n",
        "    Args:\n",
        "        input_sentence_tensor: TensorFlow tensor containing the padded\n",
        "                               tokenized English sentence.\n",
        "        transformer: The trained Transformer model.\n",
        "\n",
        "    Returns:\n",
        "        The translated Hindi sentence string.\n",
        "    \"\"\"\n",
        "    # Get the integer ID for the start token in the Hindi tokenizer\n",
        "    # We assume '<start>' and '<end>' tokens were added during tokenization.\n",
        "    # If not, the first token added by the tokenizer (usually '<OOV>')\n",
        "    # or a specific index (like 1 if 0 is padding) might be used as start.\n",
        "    # Based on the previous tokenization, the start token isn't explicitly handled,\n",
        "    # so we'll add a dummy start token logic here. In a real scenario,\n",
        "    # you'd need to modify the tokenization step to include <start> and <end> tokens.\n",
        "    # For this example, let's assume the first token in the Hindi vocabulary\n",
        "    # after padding (index 1) could represent a 'start' or the first actual word.\n",
        "    # However, a proper implementation requires explicit start/end tokens in vocabulary.\n",
        "    # Let's refine this: The standard Transformer expects <start> and <end> tokens.\n",
        "    # Let's assume we add '<start>' and '<end>' to the vocabularies manually for demonstration.\n",
        "    # For the current data, let's simulate start with the first predicted token and\n",
        "    # end with padding (0) or a designated end token if we had one.\n",
        "    # Given the current tokenization doesn't include <start>/<end>, we'll need to adapt.\n",
        "    # A common approach for inference without explicit start/end in input vocab\n",
        "    # is to start with a special start token in the target language.\n",
        "    # Since we don't have explicit <start>/<end> in the current `hin_tokenizer`,\n",
        "    # we'll use a placeholder index (like 0, although 0 is usually padding) or\n",
        "    # assume the first predicted non-padding token is the start. This is not ideal.\n",
        "\n",
        "    # Let's make an assumption for demonstration: Add '<start>' and '<end>' manually.\n",
        "    # In a production system, modify the tokenizer setup.\n",
        "    # Assuming '<start>' has index `hin_vocab_size` and '<end>' has `hin_vocab_size + 1`\n",
        "    # This requires increasing `hin_vocab_size` by 2 and refitting/updating tokenizer.\n",
        "    # Let's use a simplified approach based on the *current* tokenizer state.\n",
        "    # We need a starting sequence for the decoder. This sequence usually begins\n",
        "    # with a designated start token.\n",
        "    # Without dedicated start/end tokens, let's use a workaround for demonstration.\n",
        "    # We need an index that *isn't* padding (0). The next available index is 1.\n",
        "    # Let's assume index 1 acts as a \"start-like\" token for this limited example.\n",
        "    # **Note:** This is a simplification for this exercise due to the limited dataset\n",
        "    # and tokenizer setup. Proper <start> and <end> tokens are standard.\n",
        "\n",
        "    # Initialize the decoder input with a start token (using index 1 as a placeholder)\n",
        "    # In a real scenario, this would be `hin_tokenizer.word_index['<start>']`\n",
        "    decoder_input = tf.expand_dims([1], axis=0) # Shape: (1, 1)\n",
        "\n",
        "    # Set a maximum translation length\n",
        "    max_length = pe_target # Use the target positional encoding length\n",
        "\n",
        "    # Store the generated token IDs\n",
        "    output_sequence = []\n",
        "\n",
        "    # Start the greedy decoding loop\n",
        "    for i in range(max_length):\n",
        "        # Create look-ahead mask for the decoder input\n",
        "        look_ahead_mask = create_look_ahead_mask(tf.shape(decoder_input)[1])\n",
        "        # Create padding mask for the decoder input\n",
        "        dec_padding_mask = create_padding_mask(decoder_input)\n",
        "\n",
        "        # Combine masks (look-ahead and padding on the decoder side)\n",
        "        combined_mask = tf.maximum(look_ahead_mask, dec_padding_mask)\n",
        "\n",
        "\n",
        "        # Get predictions from the transformer\n",
        "        # The transformer's call method expects: inp, tar, look_ahead_mask, dec_padding_mask\n",
        "        # Here, inp is the encoder input, tar is the current decoder input\n",
        "        predictions = transformer(\n",
        "            inp=input_sentence_tensor,\n",
        "            tar=decoder_input,\n",
        "            look_ahead_mask=combined_mask, # Use the combined mask for decoder self-attention\n",
        "            dec_padding_mask=create_padding_mask(input_sentence_tensor) # Padding mask for encoder-decoder attention\n",
        "        )\n",
        "\n",
        "        # Get the prediction for the next token (from the last time step)\n",
        "        predictions = predictions[:, -1, :]  # Shape: (batch_size, target_vocab_size)\n",
        "        predicted_id = tf.argmax(predictions, axis=-1).numpy()[0] # Get the token ID (scalar)\n",
        "\n",
        "        # Check if the predicted token is the end token\n",
        "        # We don't have an explicit end token, let's use padding (0) as a proxy\n",
        "        # or a specific high index if we had added one.\n",
        "        # Let's assume for this limited example that index 0 (padding) acts like an end token.\n",
        "        # **Note:** This is another simplification. Proper <end> tokens are standard.\n",
        "        if predicted_id == 0: # Assuming 0 is the padding/end token\n",
        "             break\n",
        "\n",
        "        # Append the predicted token ID to the output sequence\n",
        "        output_sequence.append(predicted_id)\n",
        "\n",
        "        # Concatenate the predicted ID to the decoder input for the next step\n",
        "        decoder_input = tf.concat([decoder_input, tf.expand_dims([predicted_id], axis=0)], axis=-1)\n",
        "\n",
        "    # Convert the sequence of predicted IDs back to a Hindi sentence\n",
        "    # The tokenizer's `sequences_to_texts` method expects a list of sequences, so wrap the output list.\n",
        "    # We need to handle potential OOV tokens and the placeholder start token if it was added.\n",
        "    # The `sequences_to_texts` method handles converting indices back to words.\n",
        "    # It will ignore padding (0) by default.\n",
        "    translated_sentence = hin_tokenizer.sequences_to_texts([output_sequence])\n",
        "\n",
        "    # `sequences_to_texts` returns a list of strings (one string per sequence).\n",
        "    # We only have one sequence, so take the first element.\n",
        "    return translated_sentence[0]\n",
        "\n",
        "# Helper function for creating the look-ahead mask\n",
        "def create_look_ahead_mask(size):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask  # (seq_len, seq_len)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99a2d535"
      },
      "source": [
        "## Use the translation function with a sample english sentence\n",
        "\n",
        "### Subtask:\n",
        "Translate a sample English sentence using the `preprocess_sentence` and `translate` functions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e88721a"
      },
      "source": [
        "**Reasoning**:\n",
        "Define a sample English sentence, preprocess it using the `preprocess_sentence` function, and then use the `translate` function with the transformer model to get the Hindi translation. Finally, print both the original and translated sentences.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4b202c1e",
        "outputId": "85665f42-9c74-414e-8bbb-640bc1707ba3"
      },
      "source": [
        "# 1. Define a sample English sentence\n",
        "sample_english_sentence = \"how are you\"\n",
        "\n",
        "# 2. Call the preprocess_sentence function\n",
        "input_tensor = preprocess_sentence(sample_english_sentence)\n",
        "\n",
        "# 3. Call the translate function with the input tensor and the transformer model\n",
        "translated_hindi_sentence = translate(input_tensor, transformer)\n",
        "\n",
        "# 4. Print the original and translated sentences\n",
        "print(\"Original English:\", sample_english_sentence)\n",
        "print(\"Translated Hindi:\", translated_hindi_sentence)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of q in scaled_dot_product_attention: tf.Tensor([ 1  8  4 16], shape=(4,), dtype=int32)\n",
            "Shape of k in scaled_dot_product_attention: tf.Tensor([ 1  8  4 16], shape=(4,), dtype=int32)\n",
            "Shape of v in scaled_dot_product_attention: tf.Tensor([ 1  8  4 16], shape=(4,), dtype=int32)\n",
            "Shape of mask in scaled_dot_product_attention: tf.Tensor([1 1 1 4], shape=(4,), dtype=int32)\n",
            "Shape of q in scaled_dot_product_attention: tf.Tensor([ 1  8  4 16], shape=(4,), dtype=int32)\n",
            "Shape of k in scaled_dot_product_attention: tf.Tensor([ 1  8  4 16], shape=(4,), dtype=int32)\n",
            "Shape of v in scaled_dot_product_attention: tf.Tensor([ 1  8  4 16], shape=(4,), dtype=int32)\n",
            "Shape of mask in scaled_dot_product_attention: tf.Tensor([1 1 1 4], shape=(4,), dtype=int32)\n",
            "Shape of q in scaled_dot_product_attention: tf.Tensor([ 1  8  1 16], shape=(4,), dtype=int32)\n",
            "Shape of k in scaled_dot_product_attention: tf.Tensor([ 1  8  1 16], shape=(4,), dtype=int32)\n",
            "Shape of v in scaled_dot_product_attention: tf.Tensor([ 1  8  1 16], shape=(4,), dtype=int32)\n",
            "Shape of mask in scaled_dot_product_attention: tf.Tensor([1 1 1 1], shape=(4,), dtype=int32)\n",
            "Shape of q in scaled_dot_product_attention: tf.Tensor([ 1  8  1 16], shape=(4,), dtype=int32)\n",
            "Shape of k in scaled_dot_product_attention: tf.Tensor([ 1  8  4 16], shape=(4,), dtype=int32)\n",
            "Shape of v in scaled_dot_product_attention: tf.Tensor([ 1  8  4 16], shape=(4,), dtype=int32)\n",
            "Shape of mask in scaled_dot_product_attention: tf.Tensor([1 1 1 4], shape=(4,), dtype=int32)\n",
            "Shape of q in scaled_dot_product_attention: tf.Tensor([ 1  8  1 16], shape=(4,), dtype=int32)\n",
            "Shape of k in scaled_dot_product_attention: tf.Tensor([ 1  8  1 16], shape=(4,), dtype=int32)\n",
            "Shape of v in scaled_dot_product_attention: tf.Tensor([ 1  8  1 16], shape=(4,), dtype=int32)\n",
            "Shape of mask in scaled_dot_product_attention: tf.Tensor([1 1 1 1], shape=(4,), dtype=int32)\n",
            "Shape of q in scaled_dot_product_attention: tf.Tensor([ 1  8  1 16], shape=(4,), dtype=int32)\n",
            "Shape of k in scaled_dot_product_attention: tf.Tensor([ 1  8  4 16], shape=(4,), dtype=int32)\n",
            "Shape of v in scaled_dot_product_attention: tf.Tensor([ 1  8  4 16], shape=(4,), dtype=int32)\n",
            "Shape of mask in scaled_dot_product_attention: tf.Tensor([1 1 1 4], shape=(4,), dtype=int32)\n",
            "Shape of q in scaled_dot_product_attention: tf.Tensor([ 1  8  4 16], shape=(4,), dtype=int32)\n",
            "Shape of k in scaled_dot_product_attention: tf.Tensor([ 1  8  4 16], shape=(4,), dtype=int32)\n",
            "Shape of v in scaled_dot_product_attention: tf.Tensor([ 1  8  4 16], shape=(4,), dtype=int32)\n",
            "Shape of mask in scaled_dot_product_attention: tf.Tensor([1 1 1 4], shape=(4,), dtype=int32)\n",
            "Shape of q in scaled_dot_product_attention: tf.Tensor([ 1  8  4 16], shape=(4,), dtype=int32)\n",
            "Shape of k in scaled_dot_product_attention: tf.Tensor([ 1  8  4 16], shape=(4,), dtype=int32)\n",
            "Shape of v in scaled_dot_product_attention: tf.Tensor([ 1  8  4 16], shape=(4,), dtype=int32)\n",
            "Shape of mask in scaled_dot_product_attention: tf.Tensor([1 1 1 4], shape=(4,), dtype=int32)\n",
            "Shape of q in scaled_dot_product_attention: tf.Tensor([ 1  8  2 16], shape=(4,), dtype=int32)\n",
            "Shape of k in scaled_dot_product_attention: tf.Tensor([ 1  8  2 16], shape=(4,), dtype=int32)\n",
            "Shape of v in scaled_dot_product_attention: tf.Tensor([ 1  8  2 16], shape=(4,), dtype=int32)\n",
            "Shape of mask in scaled_dot_product_attention: tf.Tensor([1 1 2 2], shape=(4,), dtype=int32)\n",
            "Shape of q in scaled_dot_product_attention: tf.Tensor([ 1  8  2 16], shape=(4,), dtype=int32)\n",
            "Shape of k in scaled_dot_product_attention: tf.Tensor([ 1  8  4 16], shape=(4,), dtype=int32)\n",
            "Shape of v in scaled_dot_product_attention: tf.Tensor([ 1  8  4 16], shape=(4,), dtype=int32)\n",
            "Shape of mask in scaled_dot_product_attention: tf.Tensor([1 1 1 4], shape=(4,), dtype=int32)\n",
            "Shape of q in scaled_dot_product_attention: tf.Tensor([ 1  8  2 16], shape=(4,), dtype=int32)\n",
            "Shape of k in scaled_dot_product_attention: tf.Tensor([ 1  8  2 16], shape=(4,), dtype=int32)\n",
            "Shape of v in scaled_dot_product_attention: tf.Tensor([ 1  8  2 16], shape=(4,), dtype=int32)\n",
            "Shape of mask in scaled_dot_product_attention: tf.Tensor([1 1 2 2], shape=(4,), dtype=int32)\n",
            "Shape of q in scaled_dot_product_attention: tf.Tensor([ 1  8  2 16], shape=(4,), dtype=int32)\n",
            "Shape of k in scaled_dot_product_attention: tf.Tensor([ 1  8  4 16], shape=(4,), dtype=int32)\n",
            "Shape of v in scaled_dot_product_attention: tf.Tensor([ 1  8  4 16], shape=(4,), dtype=int32)\n",
            "Shape of mask in scaled_dot_product_attention: tf.Tensor([1 1 1 4], shape=(4,), dtype=int32)\n",
            "Shape of q in scaled_dot_product_attention: tf.Tensor([ 1  8  4 16], shape=(4,), dtype=int32)\n",
            "Shape of k in scaled_dot_product_attention: tf.Tensor([ 1  8  4 16], shape=(4,), dtype=int32)\n",
            "Shape of v in scaled_dot_product_attention: tf.Tensor([ 1  8  4 16], shape=(4,), dtype=int32)\n",
            "Shape of mask in scaled_dot_product_attention: tf.Tensor([1 1 1 4], shape=(4,), dtype=int32)\n",
            "Shape of q in scaled_dot_product_attention: tf.Tensor([ 1  8  4 16], shape=(4,), dtype=int32)\n",
            "Shape of k in scaled_dot_product_attention: tf.Tensor([ 1  8  4 16], shape=(4,), dtype=int32)\n",
            "Shape of v in scaled_dot_product_attention: tf.Tensor([ 1  8  4 16], shape=(4,), dtype=int32)\n",
            "Shape of mask in scaled_dot_product_attention: tf.Tensor([1 1 1 4], shape=(4,), dtype=int32)\n",
            "Shape of q in scaled_dot_product_attention: tf.Tensor([ 1  8  3 16], shape=(4,), dtype=int32)\n",
            "Shape of k in scaled_dot_product_attention: tf.Tensor([ 1  8  3 16], shape=(4,), dtype=int32)\n",
            "Shape of v in scaled_dot_product_attention: tf.Tensor([ 1  8  3 16], shape=(4,), dtype=int32)\n",
            "Shape of mask in scaled_dot_product_attention: tf.Tensor([1 1 3 3], shape=(4,), dtype=int32)\n",
            "Shape of q in scaled_dot_product_attention: tf.Tensor([ 1  8  3 16], shape=(4,), dtype=int32)\n",
            "Shape of k in scaled_dot_product_attention: tf.Tensor([ 1  8  4 16], shape=(4,), dtype=int32)\n",
            "Shape of v in scaled_dot_product_attention: tf.Tensor([ 1  8  4 16], shape=(4,), dtype=int32)\n",
            "Shape of mask in scaled_dot_product_attention: tf.Tensor([1 1 1 4], shape=(4,), dtype=int32)\n",
            "Shape of q in scaled_dot_product_attention: tf.Tensor([ 1  8  3 16], shape=(4,), dtype=int32)\n",
            "Shape of k in scaled_dot_product_attention: tf.Tensor([ 1  8  3 16], shape=(4,), dtype=int32)\n",
            "Shape of v in scaled_dot_product_attention: tf.Tensor([ 1  8  3 16], shape=(4,), dtype=int32)\n",
            "Shape of mask in scaled_dot_product_attention: tf.Tensor([1 1 3 3], shape=(4,), dtype=int32)\n",
            "Shape of q in scaled_dot_product_attention: tf.Tensor([ 1  8  3 16], shape=(4,), dtype=int32)\n",
            "Shape of k in scaled_dot_product_attention: tf.Tensor([ 1  8  4 16], shape=(4,), dtype=int32)\n",
            "Shape of v in scaled_dot_product_attention: tf.Tensor([ 1  8  4 16], shape=(4,), dtype=int32)\n",
            "Shape of mask in scaled_dot_product_attention: tf.Tensor([1 1 1 4], shape=(4,), dtype=int32)\n",
            "Shape of q in scaled_dot_product_attention: tf.Tensor([ 1  8  4 16], shape=(4,), dtype=int32)\n",
            "Shape of k in scaled_dot_product_attention: tf.Tensor([ 1  8  4 16], shape=(4,), dtype=int32)\n",
            "Shape of v in scaled_dot_product_attention: tf.Tensor([ 1  8  4 16], shape=(4,), dtype=int32)\n",
            "Shape of mask in scaled_dot_product_attention: tf.Tensor([1 1 1 4], shape=(4,), dtype=int32)\n",
            "Shape of q in scaled_dot_product_attention: tf.Tensor([ 1  8  4 16], shape=(4,), dtype=int32)\n",
            "Shape of k in scaled_dot_product_attention: tf.Tensor([ 1  8  4 16], shape=(4,), dtype=int32)\n",
            "Shape of v in scaled_dot_product_attention: tf.Tensor([ 1  8  4 16], shape=(4,), dtype=int32)\n",
            "Shape of mask in scaled_dot_product_attention: tf.Tensor([1 1 1 4], shape=(4,), dtype=int32)\n",
            "Shape of q in scaled_dot_product_attention: tf.Tensor([ 1  8  4 16], shape=(4,), dtype=int32)\n",
            "Shape of k in scaled_dot_product_attention: tf.Tensor([ 1  8  4 16], shape=(4,), dtype=int32)\n",
            "Shape of v in scaled_dot_product_attention: tf.Tensor([ 1  8  4 16], shape=(4,), dtype=int32)\n",
            "Shape of mask in scaled_dot_product_attention: tf.Tensor([1 1 4 4], shape=(4,), dtype=int32)\n",
            "Shape of q in scaled_dot_product_attention: tf.Tensor([ 1  8  4 16], shape=(4,), dtype=int32)\n",
            "Shape of k in scaled_dot_product_attention: tf.Tensor([ 1  8  4 16], shape=(4,), dtype=int32)\n",
            "Shape of v in scaled_dot_product_attention: tf.Tensor([ 1  8  4 16], shape=(4,), dtype=int32)\n",
            "Shape of mask in scaled_dot_product_attention: tf.Tensor([1 1 1 4], shape=(4,), dtype=int32)\n",
            "Shape of q in scaled_dot_product_attention: tf.Tensor([ 1  8  4 16], shape=(4,), dtype=int32)\n",
            "Shape of k in scaled_dot_product_attention: tf.Tensor([ 1  8  4 16], shape=(4,), dtype=int32)\n",
            "Shape of v in scaled_dot_product_attention: tf.Tensor([ 1  8  4 16], shape=(4,), dtype=int32)\n",
            "Shape of mask in scaled_dot_product_attention: tf.Tensor([1 1 4 4], shape=(4,), dtype=int32)\n",
            "Shape of q in scaled_dot_product_attention: tf.Tensor([ 1  8  4 16], shape=(4,), dtype=int32)\n",
            "Shape of k in scaled_dot_product_attention: tf.Tensor([ 1  8  4 16], shape=(4,), dtype=int32)\n",
            "Shape of v in scaled_dot_product_attention: tf.Tensor([ 1  8  4 16], shape=(4,), dtype=int32)\n",
            "Shape of mask in scaled_dot_product_attention: tf.Tensor([1 1 1 4], shape=(4,), dtype=int32)\n",
            "Original English: how are you\n",
            "Translated Hindi: कैसे कैसे कैसे कैसे\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5878afba"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   A Python function `preprocess_sentence` was successfully created to tokenize and pad an English sentence to a fixed length using a pre-fitted English tokenizer.\n",
        "*   A Python function `translate` was successfully created to perform greedy decoding using the trained Transformer model. This function takes the preprocessed English input, iteratively predicts the next Hindi token, and stops based on a maximum length or a designated end token (simulated by padding in this example).\n",
        "*   The `translate` function utilizes look-ahead and padding masks during the decoding process.\n",
        "*   A sample English sentence \"how are you\" was preprocessed and translated using the created functions and the Transformer model.\n",
        "*   The translation output for \"how are you\" was \"कैसे कैसे कैसे कैसे\".\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The current implementation of the `translate` function uses padding (index 0) as a proxy for the end token. A more robust approach would involve explicitly adding `<start>` and `<end>` tokens to the Hindi vocabulary and modifying the tokenizer accordingly.\n",
        "*   The model's translation output \"कैसे कैसे कैसे कैसे\" for \"how are you\" suggests potential issues with the training data, model capacity, or training process, as it is a repetitive translation rather than a meaningful one. Further investigation into the training data and model evaluation metrics is recommended.\n"
      ]
    }
  ]
}